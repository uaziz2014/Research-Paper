{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    }
   },
   "source": [
    "# Use a pipeline as a high-level helper\n",
    "from transformers import pipeline\n",
    "\n",
    "#device = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "#print(device)\n",
    "\n",
    "classifier = pipeline(\"text-classification\", model=\"Hate-speech-CNERG/bert-base-uncased-hatexplain\")\n",
    "#, device=device\n",
    "#original_text = \"and this is why I end up with nigger trainee doctors who can not speak properly lack basic knowledge\"\n",
    "\n",
    "#classifier(original_text, return_all_scores=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "json_file = './dataset.json'\n",
    "\n",
    "def extract_sentences(json_file):\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        sentences = []\n",
    "        for key, entry in data.items():\n",
    "            \n",
    "            if 'post_tokens' in entry:\n",
    "                post_tokens = entry['post_tokens']\n",
    "                sentence = \" \".join(post_tokens)\n",
    "                sentences.append(sentence)\n",
    "            else:\n",
    "                print(f\"Warning: Entry {key} is missing 'post_tokens' key\")\n",
    "\n",
    "        return sentences\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File {json_file} not found.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Error: File {json_file} is not a valid JSON.\")\n",
    "        return []\n",
    "\n",
    "sentences = extract_sentences(json_file)\n",
    "#print(len(sentences))\n",
    "#print(sentences[:5]) "
   ],
   "id": "a6ecca1e7778febe"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-06-28T11:46:35.384958Z",
     "start_time": "2024-06-28T11:46:30.314160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import shap\n",
    "explainer = shap.Explainer(classifier)\n",
    "shap_values = explainer(sentences[:30])"
   ],
   "id": "70bdd8f40f1217cf",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classifier' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mshap\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m explainer \u001B[38;5;241m=\u001B[39m shap\u001B[38;5;241m.\u001B[39mExplainer(\u001B[43mclassifier\u001B[49m)\n\u001B[0;32m      3\u001B[0m shap_values \u001B[38;5;241m=\u001B[39m explainer(sentences[:\u001B[38;5;241m30\u001B[39m])\n",
      "\u001B[1;31mNameError\u001B[0m: name 'classifier' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "#shap.plots.text(shap_values, display=True)\n",
    "#print(shap_values.values[0])\n",
    "#print(shap_values.data[0])\n",
    "print(shap_values[0])\n",
    "\n",
    "def shap_to_natural_language(shap_values, top_n=3, class_index=1):\n",
    "    explanations = []\n",
    "    for i in range(len(shap_values)):\n",
    "        sentence = shap_values.data[i]\n",
    "        sentence_shap_values = shap_values.values[i]\n",
    "        explanation = f\"{i + 1}. Sentence: '{' '.join(sentence)}'\\nThe model predicted this because:\"\n",
    "        # 获取特征和值的对\n",
    "        #word_importance = list(zip(sentence, sentence_shap_values[:, class_index]))\n",
    "        # 按照SHAP值排序，并选择前top_n个\n",
    "        #word_importance.sort(key=lambda x: abs(x[1]), reverse=True)\n",
    "        #top_words = word_importance[:top_n]\n",
    "        #for word, value in top_words:\n",
    "        lists = list(zip(sentence, sentence_shap_values[:, class_index]))\n",
    "        for word, value in lists:\n",
    "            explanation += f\"\\n- The word '{word}' has a SHAP value of {value:.4f}\"\n",
    "        explanations.append(explanation)\n",
    "    return explanations\n",
    "\n",
    "\n",
    "explanations = shap_to_natural_language(shap_values, sentences)\n",
    "for explanation in explanations:\n",
    "    print(explanation)\n",
    "    print(\"\\n\")"
   ],
   "id": "eb6e82bd16db3d60"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

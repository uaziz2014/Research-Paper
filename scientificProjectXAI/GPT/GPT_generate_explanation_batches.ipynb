{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-28T20:45:54.674123Z",
     "start_time": "2024-08-28T20:45:53.124902Z"
    }
   },
   "source": [
    "import json\n",
    "\n",
    "with open(\"../data/annotated_labels.json\", \"r\") as f:\n",
    "    annotated_labels = json.load(f)\n",
    "\n",
    "with open(\"../data/classified_labels.json\", \"r\") as f:\n",
    "    classified_labels = json.load(f)\n",
    "\n",
    "with open('../data/original_post_tokens.json', 'r', encoding='utf-8') as f:\n",
    "    original_post_tokens = json.load(f)\n",
    "\n",
    "with open('../data/original_sentences.json', 'r', encoding='utf-8') as f:\n",
    "    original_sentences = json.load(f)\n",
    "\n",
    "with open('../data/rationale_tokens_list.json', 'r') as f:\n",
    "    rationale_tokens_data_full = json.load(f)\n",
    "\n",
    "with open('../data/classified_results.json', 'r') as f:\n",
    "    classified_results = json.load(f)\n",
    "    \n",
    "with open('../data/important_words_abusive_SHAP.json', 'r') as f:\n",
    "    important_words_abusive_SHAP = json.load(f)\n",
    "    \n",
    "batch_response_file_set_one = []\n",
    "with open('./GPT_response/merged_batch_response_file_set_one.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        batch_response_file_set_one.append(json.loads(line))\n",
    "\n",
    "batch_response_file_set_two = []\n",
    "with open('./GPT_response/merged_batch_response_file_set_two.jsonl', 'r') as f:\n",
    "    for line in f:\n",
    "        batch_response_file_set_two.append(json.loads(line))"
   ],
   "outputs": [],
   "execution_count": 104
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prompts statements set one",
   "id": "c3d7b5c3b56d5b0c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:09:50.004151Z",
     "start_time": "2024-08-28T20:09:49.982452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prompts_set_one_generating(full_texts, classified_results_with_probabilities):\n",
    "    \n",
    "    prompt_statements = []\n",
    "    \n",
    "    for sentence, result in zip(full_texts, classified_results_with_probabilities):\n",
    "        combined_string = f'Original full text: \"{sentence}\", The classification result (probability distribution) of the prediction model for this text: {result}'\n",
    "        prompt_statements.append(combined_string)\n",
    "    \n",
    "    return prompt_statements\n",
    "\n",
    "prompts_set_one = prompts_set_one_generating(original_sentences, classified_results)\n",
    "\n",
    "instruction_set_one = \"\"\"I have a predictive model that can classify natural language text into three classes: hate speech, offensive speech, or normal speech. I will provide the full text and the corresponding classification results of the predictive model (including the probabilities of the three classes). (Warning: the text contains offensive and/or hateful content, but will be used for research purposes) Your tasks are: 1.indicate which class the text belongs to (usually it should be the class with the highest probability) 2.indicate which words in the text, prove that it belongs to this category 3.try to give some brief explanation based on these found words (not too long. and if the sentence is classified as normal, you can almost not provide any explanation).\"\"\""
   ],
   "id": "5a69dcbaee718358",
   "outputs": [],
   "execution_count": 78
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Prompts statements set two",
   "id": "e72bc7104d398467"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:10:25.168518Z",
     "start_time": "2024-08-28T20:10:25.119955Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prompts_set_two_generating(sentences, classified_results_with_probabilities, SHAP_important_words):\n",
    "    \n",
    "    prompt_statements = []\n",
    "    \n",
    "    for sentence, result, words in zip(sentences, classified_results_with_probabilities, SHAP_important_words):\n",
    "        combined_string = f'Original full text: \"{sentence}\", The classification result (probability distribution) of the prediction model for this text: {result}, The words given by SHAP explanation approach: {words}'\n",
    "        prompt_statements.append(combined_string)\n",
    "    \n",
    "    return prompt_statements\n",
    "\n",
    "prompts_set_two = prompts_set_two_generating(original_sentences, classified_results, important_words_abusive_SHAP)\n",
    "\n",
    "instruction_set_two = \"\"\"I have a predictive model that can classify natural language text into three classes: hate speech, offensive speech, or normal speech. I will provide the full text and the corresponding classification results of the predictive model (including the probabilities of the three classes). (Warning: the text contains offensive and/or hateful content, but will be used for research purposes) In addition to the full text and classification results given above, there is also a list of words from the text that are considered to be the most important indicators pointing to the classification results, obtained using the SHAP explanation approach. Your tasks are: 1.indicate which class the text belongs to (usually it should be the class with the highest probability) 2.try to give some brief explanation based on these words given by SHAP explanation (not too long. and if the sentence is classified as normal, you can almost not provide any explanation)\"\"\""
   ],
   "id": "c216cc75efae4f92",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Sent message/prompts to openai model, then receive example message",
   "id": "d870d6e2be792414"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class ClassifiedResultsExplanation(BaseModel):\n",
    "    classified_class: str\n",
    "    words_with_inappropriate_meanings: list[str]\n",
    "    explanation: str\n",
    "\n",
    "completion = client.beta.chat.completions.parse(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    messages=[\n",
    "      {\"role\": \"system\", \"content\": instruction_set_two},\n",
    "      {\"role\": \"user\", \"content\": selected_prompts_set_two[2]}\n",
    "    ],\n",
    "    response_format={\n",
    "        \"type\": \"json_schema\",\n",
    "        \"json_schema\": {\n",
    "            \"name\": \"PromptSet1ResponseFormat\",\n",
    "            \"schema\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"classified_class\": {\"type\": \"string\"},\n",
    "                    \"words_with_inappropriate_meanings\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    \"explanation\": {\"type\": \"string\"}\n",
    "                },\n",
    "                \"required\": [\"classified_class\", \"words_with_inappropriate_meanings\", \"explanation\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            \"strict\": True\n",
    "        }\n",
    "    }\n",
    ")\n",
    "    \n",
    "content = completion.choices[0].message.content\n",
    "\n",
    "parsed_content = json.loads(content)\n",
    "\n",
    "result = ClassifiedResultsExplanation(**parsed_content)\n",
    "\n",
    "print(result.classified_class)"
   ],
   "id": "93bd273b98717d43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(result.classified_class)",
   "id": "7bc5801a3be6a2e1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(result.words_with_inappropriate_meanings)",
   "id": "2380be91c55c94d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "print(result.explanation)",
   "id": "7b199c10d3b81fef",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Create batch files",
   "id": "6373e0c37c02406e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:11:07.377657Z",
     "start_time": "2024-08-28T20:11:05.432407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "\n",
    "split_size = 1000\n",
    "\n",
    "# class PromptSet1ResponseFormat(BaseModel):\n",
    "#     classified_class: str\n",
    "#     words_with_inappropriate_meanings: list[str]\n",
    "#     explanation: str\n",
    "    \n",
    "for j in range(0, len(prompts_set_one), split_size):\n",
    "    with open(f\"./GPT_prompts/gpt_batch_prompts_set_one_part{j//split_size + 1}.jsonl\", \"w\") as file:\n",
    "        for i, prompt in enumerate(prompts_set_one[j:j+split_size]):\n",
    "            entry = {\n",
    "                \"custom_id\": f\"request-{j+i+1}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": instruction_set_one},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    \"response_format\": {\n",
    "                        \"type\": \"json_schema\",\n",
    "                        \"json_schema\": {\n",
    "                            \"name\": \"PromptSet1ResponseFormat\",\n",
    "                            \"schema\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"classified_class\": {\"type\": \"string\"},\n",
    "                                    \"words_with_inappropriate_meanings\": {\n",
    "                                        \"type\": \"array\",\n",
    "                                        \"items\": {\"type\": \"string\"}\n",
    "                                    },\n",
    "                                    \"explanation\": {\"type\": \"string\"}\n",
    "                                },\n",
    "                                \"required\": [\"classified_class\", \"words_with_inappropriate_meanings\", \"explanation\"],\n",
    "                                \"additionalProperties\": False\n",
    "                            },\n",
    "                            \"strict\": True\n",
    "                        }\n",
    "                    },\n",
    "                    \"max_tokens\": 1000\n",
    "                }\n",
    "            }\n",
    "            file.write(json.dumps(entry) + \"\\n\")\n",
    "\n",
    "# class PromptSet2ResponseFormat(BaseModel):\n",
    "#     classified_class: str\n",
    "#     explanation: str\n",
    "    \n",
    "for j in range(0, len(prompts_set_two), split_size):\n",
    "    with open(f\"./GPT_prompts/gpt_batch_prompts_set_two_part{j//split_size + 1}.jsonl\", \"w\") as file:\n",
    "        for i, prompt in enumerate(prompts_set_two[j:j+split_size]):\n",
    "            entry = {\n",
    "                \"custom_id\": f\"request-{j+i+1}\",\n",
    "                \"method\": \"POST\",\n",
    "                \"url\": \"/v1/chat/completions\",\n",
    "                \"body\": {\n",
    "                    \"model\": \"gpt-4o-mini\",\n",
    "                    \"messages\": [\n",
    "                        {\"role\": \"system\", \"content\": instruction_set_two},\n",
    "                        {\"role\": \"user\", \"content\": prompt}\n",
    "                    ],\n",
    "                    \"response_format\": {\n",
    "                        \"type\": \"json_schema\",\n",
    "                        \"json_schema\": {\n",
    "                            \"name\": \"PromptSet2ResponseFormat\",\n",
    "                            \"schema\": {\n",
    "                                \"type\": \"object\",\n",
    "                                \"properties\": {\n",
    "                                    \"classified_class\": {\"type\": \"string\"},\n",
    "                                    \"explanation\": {\"type\": \"string\"}\n",
    "                                },\n",
    "                                \"required\": [\"classified_class\", \"explanation\"],\n",
    "                                \"additionalProperties\": False\n",
    "                            },\n",
    "                            \"strict\": True\n",
    "                        }\n",
    "                    },\n",
    "                    \"max_tokens\": 1000\n",
    "                }\n",
    "            }\n",
    "            file.write(json.dumps(entry) + \"\\n\")"
   ],
   "id": "ee14726d4710316e",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Upload batch files to GPT Platform",
   "id": "f77ca1090e7020af"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:11:24.443307Z",
     "start_time": "2024-08-28T20:11:24.432339Z"
    }
   },
   "cell_type": "code",
   "source": [
    "batches_id_set_one = []\n",
    "# \"\"\"['batch_CZlr5u5MynMV5Gl9YWP9zedk', 'batch_FAb7RUUpL3MM1HWmNMyge7ko', 'batch_1ACkSpq7fEMTdYNSObGoBLLI', 'batch_wdPoZggDLKL4qVDDsdminefZ', 'batch_67YDlBYxjJXxaY0WuK0z7f6T', 'batch_FJijTM4d71NcvuQKBPZNPdBj', 'batch_o7UOgGcpEb1u0mAH2BRg0ulp', 'batch_uu4yFwZo5DnHF6k2sAIS0049', 'batch_iv05EwBOT4KXp8qTyRHfy6Rr', 'batch_1yWdpaUOWPP6GeAWIhDYdECi', 'batch_qrNUVJ0FEgh1CzVmY5SjmEDT', 'batch_YuR2JS8Hg6gY3L53sbj081NV', 'batch_pwQzLwuupNbwoY6DCpRBoYm4', 'batch_F3VsjDxF2U9qiIVhYfgGM3Gc', 'batch_HrhrDn7uh618SgYlc55Zn3MH', 'batch_nBCeMXXr3ukK5LKYcgCsKmjf']\"\"\"\n",
    "batches_id_set_two = []"
   ],
   "id": "272d2cd5ee3b5ea8",
   "outputs": [],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:55:03.774064Z",
     "start_time": "2024-08-28T18:55:01.537573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# 对于第一个集合的文件\n",
    "for i in range(21, 22):\n",
    "    file_name = f\"./GPT_prompts/gpt_batch_prompts_set_one_part{i}.jsonl\"\n",
    "    \n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(file_name, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    batch_return = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": f\"prompts_set_one_generating_from_{file_name}\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    batches_id_set_one.append(batch_return.id)\n",
    "    print(f\"Batch created for {file_name}\")"
   ],
   "id": "2dab751ac6cffb6b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch created for ./GPT_prompts/gpt_batch_prompts_set_one_part21.jsonl\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:55:08.947961Z",
     "start_time": "2024-08-28T18:55:08.944152Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(batches_id_set_one)\n",
    "print(len(batches_id_set_one))"
   ],
   "id": "f7af32376b2f5368",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_lZ11rDSNa9nK9UVYSPxDneo5', 'batch_9bJmBvnsgSOtNajkNGQJTUim', 'batch_FqyYXFzeDZ71S0icA2kHv5IH', 'batch_qglivf0N8pmXIIjrtI36YKdG', 'batch_Q4ggXmtvz5G3wWliyVfIQrx8', 'batch_BxpXGXtrs9iyBG5xvov5HHky', 'batch_6V83aP69Heph2ZXMu3vhAFln', 'batch_baTLM0L5KbSFB4RFveRTiBSR', 'batch_pa8WfaPQRChCsIQCrv9P9Ixe', 'batch_cwYdC35hV1AreJO0Th7A77Qp', 'batch_p1OThtuEIQ9kqXbSk3vEG5xx', 'batch_798dUNyvOglVyRUquj5OD23j', 'batch_x9VBNM8Q4JC5XfcXezfbw3Bm', 'batch_LeGQ7wdjTKUptcootTwXnZ7N', 'batch_OM5sigeiDVDnASWehZCkBtvA', 'batch_XQHlXtGjQJsPQ0QW8x5XLSJU', 'batch_LhWY9jpSEUG8XAUz4Lwu7RD0', 'batch_M2J9O7RYAK8LPJx59fB3eP5Z', 'batch_ktJQpOpH1chESxr4Ib0JFUnw', 'batch_ZasgWtliDbSzJ3YskzkA3PpR', 'batch_rPp8TSLt4BDtwAtCfAVu1OdA']\n",
      "21\n"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T19:17:20.939984Z",
     "start_time": "2024-08-28T19:16:35.157363Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "for i, batch_id in enumerate(batches_id_set_one):\n",
    "    retrieve_return = client.batches.retrieve(batch_id)\n",
    "    file_response = client.files.content(retrieve_return.output_file_id)\n",
    "\n",
    "    file_name = f\"./GPT_response/batch_response_file_set_one_{i+1}.jsonl\"\n",
    "    \n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(file_response.text)"
   ],
   "id": "7a7ad329bfb5fc03",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T18:25:27.227384Z",
     "start_time": "2024-08-28T18:25:27.195849Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ClassifiedResultsExplanation(BaseModel):\n",
    "    classified_class: str\n",
    "    words_with_inappropriate_meanings: list[str]\n",
    "    explanation: str\n",
    "    \n",
    "outputs = []\n",
    "\n",
    "with open(\"./GPT_response/batch_response_file_set_one_1.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        outputs.append(json.loads(line))\n",
    "\n",
    "# print(outputs[0])\n",
    "# \n",
    "# print(responses[0][\"response\"]['body']['choices'][0]['message']['content'])\n",
    "\n",
    "content = outputs[256][\"response\"]['body']['choices'][0]['message']['content']\n",
    "custom_id = outputs[256][\"custom_id\"]\n",
    "\n",
    "result = ClassifiedResultsExplanation(**json.loads(content))\n",
    "\n",
    "print(custom_id, result)"
   ],
   "id": "6499867f2d3a6f9d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request-257 classified_class='hate speech' words_with_inappropriate_meanings=['jew', 'nigger'] explanation='The text contains derogatory and dehumanizing terms that target Jewish people and compare them to another racial slur, indicating a strong expression of hate and prejudice.'\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:26:05.479011Z",
     "start_time": "2024-08-28T20:26:03.458659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "for i in range(21, 22):\n",
    "    file_name = f\"./GPT_prompts/gpt_batch_prompts_set_two_part{i}.jsonl\"\n",
    "    \n",
    "    batch_input_file = client.files.create(\n",
    "        file=open(file_name, \"rb\"),\n",
    "        purpose=\"batch\"\n",
    "    )\n",
    "\n",
    "    batch_input_file_id = batch_input_file.id\n",
    "\n",
    "    batch_return = client.batches.create(\n",
    "        input_file_id=batch_input_file_id,\n",
    "        endpoint=\"/v1/chat/completions\",\n",
    "        completion_window=\"24h\",\n",
    "        metadata={\n",
    "            \"description\": f\"prompts_set_two_generating_from_{file_name}\",\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    batches_id_set_two.append(batch_return.id)\n",
    "    print(f\"Batch created for {file_name}\")"
   ],
   "id": "2df34ba17a5a17a4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch created for ./GPT_prompts/gpt_batch_prompts_set_two_part21.jsonl\n"
     ]
    }
   ],
   "execution_count": 93
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:30:24.419057Z",
     "start_time": "2024-08-28T20:30:24.410629Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(batches_id_set_two)\n",
    "print(len(batches_id_set_two))"
   ],
   "id": "575aaa42d217f73a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['batch_P3TThNcYIpcRC0U7981bIOhX', 'batch_30CdqZGi06MZfNRUpj1i4RVD', 'batch_h5owNKbkMvXkJzRRaXfyl2Rd', 'batch_oIgTNNyLSJEzG75riLI3DYpp', 'batch_t091gmsX8mkSz69If2kRqR9K', 'batch_QJonDEfxhm8sOHEAWhoLMZzo', 'batch_19pWiXPrZ6wYRQBoXsd8xAKV', 'batch_BKeFz8c2aAn0zUsSN4oYzxw3', 'batch_Wa9mYT3seUs4b8Pi62NSKwQO', 'batch_w7NiQWSQ8r2ZKhVom7ZSKQYU', 'batch_aBk3pLmEjXWyl9DyJiOPfOF0', 'batch_qTtgARu1r8AG4OnwHx000BXN', 'batch_dqG1yd1j6RIWtkXkCu3IONId', 'batch_TrmgouVCuJ699jGaqHcf2Jlh', 'batch_fl5mQzDZ2FjczjTRWs4sbAXg', 'batch_hEiPrpsmy7h3gPL282tybHpX', 'batch_GTki1lD7IkowcdoA8uRIGfOv', 'batch_6tLdSLT4W3h9cNNMywY7B2GD', 'batch_cxPvyXly5Td2NoK9M6X5Nzx9', 'batch_IGPhtfEGAJQWGCxQv70OLPLQ', 'batch_dg0o5XRZ6L4YoS2RIKBGxC0u']\n",
      "21\n"
     ]
    }
   ],
   "execution_count": 94
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:31:00.151409Z",
     "start_time": "2024-08-28T20:30:30.848205Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "for i, batch_id in enumerate(batches_id_set_two):\n",
    "    retrieve_return = client.batches.retrieve(batch_id)\n",
    "    file_response = client.files.content(retrieve_return.output_file_id)\n",
    "\n",
    "    file_name = f\"./GPT_response/batch_response_file_set_two_{i+1}.jsonl\"\n",
    "    \n",
    "    with open(file_name, \"w\") as file:\n",
    "        file.write(file_response.text)"
   ],
   "id": "4538abb7b6e7d7ae",
   "outputs": [],
   "execution_count": 95
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:32:22.159580Z",
     "start_time": "2024-08-28T20:32:22.128682Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class ClassifiedResultsExplanation(BaseModel):\n",
    "    classified_class: str\n",
    "    # words_with_inappropriate_meanings: list[str]\n",
    "    explanation: str\n",
    "    \n",
    "outputs = []\n",
    "\n",
    "with open(\"./GPT_response/batch_response_file_set_two_5.jsonl\", \"r\") as file:\n",
    "    for line in file:\n",
    "        outputs.append(json.loads(line))\n",
    "\n",
    "# print(outputs[0])\n",
    "# \n",
    "# print(responses[0][\"response\"]['body']['choices'][0]['message']['content'])\n",
    "\n",
    "content = outputs[256][\"response\"]['body']['choices'][0]['message']['content']\n",
    "custom_id = outputs[256][\"custom_id\"]\n",
    "\n",
    "result = ClassifiedResultsExplanation(**json.loads(content))\n",
    "\n",
    "print(custom_id, result)"
   ],
   "id": "765011e9b49f2a4f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "request-4257 classified_class='hate speech' explanation=\"The presence of the word 'kike,' a derogatory term, strongly indicates that the text is classified as hate speech.\"\n"
     ]
    }
   ],
   "execution_count": 97
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:32:57.084858Z",
     "start_time": "2024-08-28T20:32:57.066100Z"
    }
   },
   "cell_type": "code",
   "source": "print(prompts_set_two[4256])",
   "id": "e9642ad66ed1e15",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original full text: \"enjoy your kike faith\", The classification result (probability distribution) of the prediction model for this text: {'label': 'hate speech', 'score': 0.9301682710647583}, {'label': 'normal', 'score': 0.013806960545480251}, {'label': 'offensive', 'score': 0.05602480471134186}, The words given by SHAP explanation approach: ['enjoy', 'your', 'kike']\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:42:19.852923Z",
     "start_time": "2024-08-28T20:42:19.532233Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_file = './GPT_response/merged_batch_response_file_set_one.jsonl'\n",
    "\n",
    "with open(output_file, 'w') as outfile:\n",
    "    for i in range(1, 22):\n",
    "        file_name = f'./GPT_response/batch_response_file_set_one_{i}.jsonl'\n",
    "\n",
    "        with open(file_name, 'r') as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "\n",
    "print(f\"Files have been merged into {output_file}\")"
   ],
   "id": "4fd57a2049d5089c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been merged into ./GPT_response/merged_batch_response_file_set_one.jsonl\n"
     ]
    }
   ],
   "execution_count": 102
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-28T20:43:30.978337Z",
     "start_time": "2024-08-28T20:43:30.688273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "output_file = './GPT_response/merged_batch_response_file_set_two.jsonl'\n",
    "\n",
    "with open(output_file, 'w') as outfile:\n",
    "    for i in range(1, 22):\n",
    "        file_name = f'./GPT_response/batch_response_file_set_two_{i}.jsonl'\n",
    "\n",
    "        with open(file_name, 'r') as infile:\n",
    "            for line in infile:\n",
    "                outfile.write(line)\n",
    "\n",
    "print(f\"Files have been merged into {output_file}\")"
   ],
   "id": "9fa005e4d547c2d1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files have been merged into ./GPT_response/merged_batch_response_file_set_two.jsonl\n"
     ]
    }
   ],
   "execution_count": 103
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

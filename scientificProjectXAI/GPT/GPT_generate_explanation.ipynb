{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import Model",
   "id": "cf8dd0f0321031a8"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-20T19:27:55.590117Z",
     "start_time": "2024-08-20T19:27:49.009495Z"
    }
   },
   "source": [
    "# from transformers import pipeline\n",
    "# classifier = pipeline(\"text-classification\", model=\"Hate-speech-CNERG/bert-base-uncased-hatexplain\", device=0)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\scientificProjectXAI\\.venv\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T08:50:42.863762Z",
     "start_time": "2024-08-21T08:50:42.285632Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "# with open(\"../SHAP_values/combined_shap_values.json\", \"r\") as f:\n",
    "#     shap_data = json.load(f)\n",
    "\n",
    "with open(\"../data/annotated_labels.json\", \"r\") as f:\n",
    "    annotated_labels = json.load(f)\n",
    "\n",
    "with open(\"../data/classified_labels.json\", \"r\") as f:\n",
    "    classified_labels = json.load(f)\n",
    "\n",
    "with open('../data/original_post_tokens.json', 'r', encoding='utf-8') as f:\n",
    "    original_post_tokens = json.load(f)\n",
    "\n",
    "with open('../data/original_sentences.json', 'r', encoding='utf-8') as f:\n",
    "    original_sentences = json.load(f)\n",
    "\n",
    "with open('../data/rationale_tokens_list.json', 'r') as f:\n",
    "    rationale_tokens_data_full = json.load(f)\n",
    "\n",
    "with open('../data/classified_results.json', 'r') as f:\n",
    "    classified_results = json.load(f)"
   ],
   "id": "d024bd8f7cecb627",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract original data from JSON file",
   "id": "20a9bc017d977f5c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T19:27:55.696547Z",
     "start_time": "2024-08-20T19:27:55.692852Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import json\n",
    "# \n",
    "# json_file = '../data/dataset.json'\n",
    "# \n",
    "# def extract_data(file):\n",
    "#     with open(file, 'r', encoding='utf-8') as f:\n",
    "#         data = json.load(f)\n",
    "# \n",
    "#     sentences = []\n",
    "#     abuse_flags = []\n",
    "#     \n",
    "#     for key, entry in data.items():\n",
    "#         if 'post_tokens' in entry:\n",
    "#             post_tokens = entry['post_tokens']\n",
    "#             sentence = \" \".join(post_tokens)\n",
    "#         else:\n",
    "#             sentence = \" \"\n",
    "#             print(f\"Warning: Entry {key} is missing 'post_tokens' key\")\n",
    "#         \n",
    "#         if 'annotators' in entry:\n",
    "#             labels = [annotator['label'] for annotator in entry['annotators']]\n",
    "#             if sum(label != \"normal\" for label in labels) >= 2:\n",
    "#                 abuse_label = 0  # Abusive\n",
    "#             else:\n",
    "#                 abuse_label = 1  # normal\n",
    "#         else:\n",
    "#             abuse_label = 0  # Default to normal if 'annotators' key is missing\n",
    "#             print(f\"Warning: Entry {key} is missing 'annotators' key\")\n",
    "#         \n",
    "#         sentences.append(sentence)\n",
    "#         abuse_flags.append(abuse_label)\n",
    "# \n",
    "#     return sentences, abuse_flags\n",
    "# \n",
    "# original_sentences, annotated_labels = extract_data(json_file) #length = 20148\n",
    "# \n",
    "# with open(\"../data/original_sentences.json\", \"w\") as f:\n",
    "#     json.dump(original_sentences, f, indent=4)"
   ],
   "id": "d7910a2f909b2f7c",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Generate the messages for GPT Assistant messages\n",
    "\n",
    "1. Do the predictions using the classify model\n",
    "2. Reshape the classified results\n",
    "3. Store the shaped messages"
   ],
   "id": "9c224c244acc2f7e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T19:34:31.150080Z",
     "start_time": "2024-08-20T19:31:43.919177Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy\n",
    "# classified_results = classifier(original_sentences, return_all_scores=True) #top 10 instances\n",
    "# print(original_sentences[0])"
   ],
   "id": "af4b2dc954f78a0a",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T19:44:16.831134Z",
     "start_time": "2024-08-20T19:44:16.827860Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(classified_results)\n",
    "# \n",
    "# for instance in classified_results:\n",
    "#     print(instance)\n",
    "    \n",
    "# classified_results_string_list = [', '.join([str(element) for element in instance]) for instance in classified_results]\n",
    "# for i in original_sentences[:3]:\n",
    "#     print(i)\n",
    "# with open(\"../data/classified_results.json\", \"w\") as f:\n",
    "#     json.dump(classified_results_string_list, f, indent=4)\n",
    "# for s in classified_results_string_list[:5]:\n",
    "#     print(s)\n",
    "# {'label': 'hate speech', 'score': 0.03392261639237404}, {'label': 'normal', 'score': 0.770871102809906}, {'label': 'offensive', 'score': 0.19520631432533264}\n",
    "# {'label': 'hate speech', 'score': 0.07430059462785721}, {'label': 'normal', 'score': 0.6370931267738342}, {'label': 'offensive', 'score': 0.2886062264442444}\n"
   ],
   "id": "fc35e3763a87bc71",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T08:50:47.313122Z",
     "start_time": "2024-08-21T08:50:47.285928Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def results_convert_to_gpt_prompt_statement(sentences, classified_results_with_probabilities):\n",
    "    # if len(sentences) != len(classified_results_with_probabilities):\n",
    "    #     raise ValueError(\"The length of original_sentences and classified_results_string_list must be the same.\")\n",
    "    \n",
    "    prompt_statements = []\n",
    "    \n",
    "    for sentence, result in zip(sentences, classified_results_with_probabilities):\n",
    "        combined_string = f'Original text: \"{sentence}\", The classification result (probability distribution) of the prediction model for this text: {result}'\n",
    "        prompt_statements.append(combined_string)\n",
    "    \n",
    "    return prompt_statements\n",
    "\n",
    "messages_prompted_to_GPT = results_convert_to_gpt_prompt_statement(original_sentences, classified_results)\n",
    "\n",
    "# for i in messages_GPT_inputs[1567:1568]:\n",
    "#     print(i)\n",
    "# Original text: \"i dont think im getting my baby them white 9 he has two white j and nikes not even touched\", Probability distribution of prediction model output: {'label': 'hate speech', 'score': 0.03392261639237404}, {'label': 'normal', 'score': 0.770871102809906}, {'label': 'offensive', 'score': 0.19520631432533264}"
   ],
   "id": "44c754f83c6518f1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GPT Assistant API",
   "id": "3ed5b3c30e7d488c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T19:27:57.783220Z",
     "start_time": "2024-08-20T19:27:56.198030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from openai import OpenAI\n",
    "# \n",
    "# client = OpenAI()\n",
    "# \n",
    "# thread = client.beta.threads.create()\n",
    "# \n",
    "# message = client.beta.threads.messages.create(\n",
    "#   thread_id=thread.id,\n",
    "#   role=\"user\",\n",
    "#   content=messages_prompted_to_GPT[4]\n",
    "# )\n",
    "\n",
    "\n",
    "# run = client.beta.threads.create_and_run(\n",
    "#   assistant_id=\"asst_JFDjSdtxCjJq7pVGm8YGxKrZ\",\n",
    "#   thread={\n",
    "#     \"messages\": [\n",
    "#       {\"role\": \"user\", \"content\": messages_GPT_inputs[4]}\n",
    "#     ]\n",
    "#   }\n",
    "# )\n",
    " \n",
    "# print(run)\n",
    "\n",
    "# completion = client.chat.completions.create(\n",
    "#     model=\"gpt-4o-mini\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "#         {\n",
    "#             \"role\": \"user\",\n",
    "#             \"content\": \"Write a haiku about recursion in programming.\"\n",
    "#         }\n",
    "#     ]\n",
    "# )\n",
    "# \n",
    "# print(completion.choices[0].message)"
   ],
   "id": "14d6199a20abe9dc",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GPT Assistant with Streaming",
   "id": "b5ea02d77d90efc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T19:27:57.786775Z",
     "start_time": "2024-08-20T19:27:57.783220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from typing_extensions import override\n",
    "# from openai import AssistantEventHandler\n",
    "#  \n",
    "# # First, we create a EventHandler class to define\n",
    "# # how we want to handle the events in the response stream.\n",
    "#  \n",
    "# class EventHandler(AssistantEventHandler):    \n",
    "#   @override\n",
    "#   def on_text_created(self, text) -> None:\n",
    "#     print(f\"\\nassistant > \", end=\"\", flush=True)\n",
    "#       \n",
    "#   @override\n",
    "#   def on_text_delta(self, delta, snapshot):\n",
    "#     print(delta.value, end=\"\", flush=True)\n",
    "#       \n",
    "#   def on_tool_call_created(self, tool_call):\n",
    "#     print(f\"\\nassistant > {tool_call.type}\\n\", flush=True)\n",
    "#   \n",
    "#   def on_tool_call_delta(self, delta, snapshot):\n",
    "#     if delta.type == 'code_interpreter':\n",
    "#       if delta.code_interpreter.input:\n",
    "#         print(delta.code_interpreter.input, end=\"\", flush=True)\n",
    "#       if delta.code_interpreter.outputs:\n",
    "#         print(f\"\\n\\noutput >\", flush=True)\n",
    "#         for output in delta.code_interpreter.outputs:\n",
    "#           if output.type == \"logs\":\n",
    "#             print(f\"\\n{output.logs}\", flush=True)\n",
    "#  \n",
    "# # Then, we use the `stream` SDK helper \n",
    "# # with the `EventHandler` class to create the Run \n",
    "# # and stream the response.\n",
    "#  \n",
    "# with client.beta.threads.runs.stream(\n",
    "#   thread_id=thread.id,\n",
    "#   assistant_id='asst_JFDjSdtxCjJq7pVGm8YGxKrZ',\n",
    "#   event_handler=EventHandler(),\n",
    "# ) as stream:\n",
    "#   stream.until_done()"
   ],
   "id": "b152a3b46bc59d27",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "GPT Assistant without Streaming",
   "id": "573399e6f8c7bdbf"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T19:28:01.450660Z",
     "start_time": "2024-08-20T19:27:57.787775Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# run = client.beta.threads.runs.create_and_poll(\n",
    "#   thread_id=thread.id,\n",
    "#   assistant_id='asst_JFDjSdtxCjJq7pVGm8YGxKrZ'\n",
    "#   # response_format='json_object'\n",
    "# )"
   ],
   "id": "2c26f0d4e90b2594",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T19:28:01.783968Z",
     "start_time": "2024-08-20T19:28:01.452655Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# if run.status == 'completed': \n",
    "# messages = client.beta.threads.messages.retrieve(\n",
    "#     thread_id=thread.id,\n",
    "#     message_id=message.id\n",
    "# )\n",
    "# print(messages)\n",
    "# else:\n",
    "#   print(run.status)\n",
    "\n",
    "# messages = client.beta.threads.messages.list(\n",
    "#     thread_id=thread.id\n",
    "# )\n",
    "# \n",
    "# response = messages.data[0].content[0].text.value\n",
    "# \n",
    "# print(response)"
   ],
   "id": "38818432805ae9db",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is classified as Offensive.\n",
      "\n",
      "The following words may have inappropriate meanings: ***bitch, hate, white, bitches***\n",
      "\n",
      "Explanation: The possible explanation is that the words/usage ***bitch*** and ***bitches*** are derogatory terms used to demean individuals, particularly women, while ***hate*** expresses strong negative feelings towards a group, and ***white*** is used in a derogatory context indicating a racial element. These usages contribute to the classification of the text as offensive.\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Loop to send prompt statements to GPT to get reply",
   "id": "c100982948353ba4"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T09:22:05.270693Z",
     "start_time": "2024-08-21T09:18:17.892211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "# thread_messages_responses = []\n",
    "run_ids = []\n",
    "\n",
    "thread = client.beta.threads.create()\n",
    "\n",
    "for message_content in messages_prompted_to_GPT[:2000]:\n",
    "    message = client.beta.threads.messages.create(\n",
    "        thread_id=thread.id,\n",
    "        role=\"user\",\n",
    "        content=message_content\n",
    "    )\n",
    "\n",
    "    run = client.beta.threads.runs.create_and_poll(\n",
    "        thread_id=thread.id,\n",
    "        assistant_id='asst_JFDjSdtxCjJq7pVGm8YGxKrZ'\n",
    "    )\n",
    "    \n",
    "    run_ids.append(run.id)\n",
    "\n",
    "    # messages = client.beta.threads.messages.list(\n",
    "    #     thread_id=thread.id\n",
    "    # )\n",
    "\n",
    "    # if run.status == 'completed':\n",
    "    #     messages = client.beta.threads.messages.list(\n",
    "    #         thread_id=thread.id\n",
    "    #     )\n",
    "        # print(messages)\n",
    "    # else:\n",
    "        # print(run.status)\n",
    "\n",
    "        # response = messages.data[0].content[0].text.value\n",
    "        # responses.append(response)"
   ],
   "id": "1d2fdfade1df393",
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': 'Thread thread_XSPoNyv5dfB5Z6LN0OFwfKCb already has an active run run_LvQEUk4iNmPtAj5BFf3WbxBu.', 'type': 'invalid_request_error', 'param': None, 'code': None}}",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mBadRequestError\u001B[0m                           Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 22\u001B[0m\n\u001B[0;32m     15\u001B[0m     message \u001B[38;5;241m=\u001B[39m client\u001B[38;5;241m.\u001B[39mbeta\u001B[38;5;241m.\u001B[39mthreads\u001B[38;5;241m.\u001B[39mmessages\u001B[38;5;241m.\u001B[39mcreate(\n\u001B[0;32m     16\u001B[0m         thread_id\u001B[38;5;241m=\u001B[39mthread\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m     17\u001B[0m         role\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124muser\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     18\u001B[0m         content\u001B[38;5;241m=\u001B[39mmessage_content\n\u001B[0;32m     19\u001B[0m     )\n\u001B[0;32m     21\u001B[0m     \u001B[38;5;66;03m# 获取 GPT 的回复\u001B[39;00m\n\u001B[1;32m---> 22\u001B[0m     run \u001B[38;5;241m=\u001B[39m \u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbeta\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mthreads\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mruns\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate_and_poll\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m        \u001B[49m\u001B[43mthread_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthread\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mid\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m        \u001B[49m\u001B[43massistant_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43masst_JFDjSdtxCjJq7pVGm8YGxKrZ\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m  \u001B[49m\u001B[38;5;66;43;03m# 使用你的 Assistant ID\u001B[39;49;00m\n\u001B[0;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m     run_ids\u001B[38;5;241m.\u001B[39mappend(run\u001B[38;5;241m.\u001B[39mid)\n\u001B[0;32m     29\u001B[0m     \u001B[38;5;66;03m# 获取所有消息（包括 GPT 的回复）\u001B[39;00m\n\u001B[0;32m     30\u001B[0m     \u001B[38;5;66;03m# messages = client.beta.threads.messages.list(\u001B[39;00m\n\u001B[0;32m     31\u001B[0m     \u001B[38;5;66;03m#     thread_id=thread.id\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     46\u001B[0m \n\u001B[0;32m     47\u001B[0m \u001B[38;5;66;03m# 输出所有回复\u001B[39;00m\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\scientificProjectXAI\\Lib\\site-packages\\openai\\resources\\beta\\threads\\runs\\runs.py:743\u001B[0m, in \u001B[0;36mRuns.create_and_poll\u001B[1;34m(self, assistant_id, additional_instructions, additional_messages, instructions, max_completion_tokens, max_prompt_tokens, metadata, model, parallel_tool_calls, response_format, temperature, tool_choice, tools, top_p, truncation_strategy, poll_interval_ms, thread_id, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    711\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcreate_and_poll\u001B[39m(\n\u001B[0;32m    712\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    713\u001B[0m     \u001B[38;5;241m*\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    736\u001B[0m     timeout: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m|\u001B[39m httpx\u001B[38;5;241m.\u001B[39mTimeout \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m|\u001B[39m NotGiven \u001B[38;5;241m=\u001B[39m NOT_GIVEN,\n\u001B[0;32m    737\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Run:\n\u001B[0;32m    738\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    739\u001B[0m \u001B[38;5;124;03m    A helper to create a run an poll for a terminal state. More information on Run\u001B[39;00m\n\u001B[0;32m    740\u001B[0m \u001B[38;5;124;03m    lifecycles can be found here:\u001B[39;00m\n\u001B[0;32m    741\u001B[0m \u001B[38;5;124;03m    https://platform.openai.com/docs/assistants/how-it-works/runs-and-run-steps\u001B[39;00m\n\u001B[0;32m    742\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 743\u001B[0m     run \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcreate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    744\u001B[0m \u001B[43m        \u001B[49m\u001B[43mthread_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mthread_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    745\u001B[0m \u001B[43m        \u001B[49m\u001B[43massistant_id\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43massistant_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    746\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_instructions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_instructions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    747\u001B[0m \u001B[43m        \u001B[49m\u001B[43madditional_messages\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43madditional_messages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    748\u001B[0m \u001B[43m        \u001B[49m\u001B[43minstructions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minstructions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    749\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    750\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmax_prompt_tokens\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmax_prompt_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    751\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    752\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    753\u001B[0m \u001B[43m        \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    754\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    755\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    756\u001B[0m \u001B[43m        \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    757\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;66;43;03m# We assume we are not streaming when polling\u001B[39;49;00m\n\u001B[0;32m    758\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    759\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtools\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    760\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    761\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    762\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    763\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    764\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    765\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    766\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    767\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpoll(\n\u001B[0;32m    768\u001B[0m         run\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m    769\u001B[0m         thread_id\u001B[38;5;241m=\u001B[39mthread_id,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    774\u001B[0m         timeout\u001B[38;5;241m=\u001B[39mtimeout,\n\u001B[0;32m    775\u001B[0m     )\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\scientificProjectXAI\\Lib\\site-packages\\openai\\_utils\\_utils.py:274\u001B[0m, in \u001B[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    272\u001B[0m             msg \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMissing required argument: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquote(missing[\u001B[38;5;241m0\u001B[39m])\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    273\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[1;32m--> 274\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\scientificProjectXAI\\Lib\\site-packages\\openai\\resources\\beta\\threads\\runs\\runs.py:495\u001B[0m, in \u001B[0;36mRuns.create\u001B[1;34m(self, thread_id, assistant_id, additional_instructions, additional_messages, instructions, max_completion_tokens, max_prompt_tokens, metadata, model, parallel_tool_calls, response_format, stream, temperature, tool_choice, tools, top_p, truncation_strategy, extra_headers, extra_query, extra_body, timeout)\u001B[0m\n\u001B[0;32m    493\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected a non-empty value for `thread_id` but received \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mthread_id\u001B[38;5;132;01m!r}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    494\u001B[0m extra_headers \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mOpenAI-Beta\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124massistants=v2\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m(extra_headers \u001B[38;5;129;01mor\u001B[39;00m {})}\n\u001B[1;32m--> 495\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_post\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    496\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/threads/\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mthread_id\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m/runs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    497\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbody\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmaybe_transform\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    498\u001B[0m \u001B[43m        \u001B[49m\u001B[43m{\u001B[49m\n\u001B[0;32m    499\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43massistant_id\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43massistant_id\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    500\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43madditional_instructions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43madditional_instructions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    501\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43madditional_messages\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43madditional_messages\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    502\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minstructions\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minstructions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    503\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_completion_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_completion_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    504\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax_prompt_tokens\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_prompt_tokens\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    505\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmetadata\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    506\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmodel\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    507\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparallel_tool_calls\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mparallel_tool_calls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    508\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mresponse_format\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mresponse_format\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    509\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mstream\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    510\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtemperature\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtemperature\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    511\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtool_choice\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtool_choice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    512\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtools\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtools\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    513\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtop_p\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtop_p\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    514\u001B[0m \u001B[43m            \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtruncation_strategy\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mtruncation_strategy\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    515\u001B[0m \u001B[43m        \u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    516\u001B[0m \u001B[43m        \u001B[49m\u001B[43mrun_create_params\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mRunCreateParams\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    517\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    518\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmake_request_options\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    519\u001B[0m \u001B[43m        \u001B[49m\u001B[43mextra_headers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_headers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_query\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_query\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mextra_body\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mextra_body\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtimeout\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtimeout\u001B[49m\n\u001B[0;32m    520\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    521\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mRun\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    522\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m    523\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mStream\u001B[49m\u001B[43m[\u001B[49m\u001B[43mAssistantStreamEvent\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    524\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\scientificProjectXAI\\Lib\\site-packages\\openai\\_base_client.py:1260\u001B[0m, in \u001B[0;36mSyncAPIClient.post\u001B[1;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1246\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpost\u001B[39m(\n\u001B[0;32m   1247\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m   1248\u001B[0m     path: \u001B[38;5;28mstr\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1255\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1256\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[0;32m   1257\u001B[0m     opts \u001B[38;5;241m=\u001B[39m FinalRequestOptions\u001B[38;5;241m.\u001B[39mconstruct(\n\u001B[0;32m   1258\u001B[0m         method\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpost\u001B[39m\u001B[38;5;124m\"\u001B[39m, url\u001B[38;5;241m=\u001B[39mpath, json_data\u001B[38;5;241m=\u001B[39mbody, files\u001B[38;5;241m=\u001B[39mto_httpx_files(files), \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions\n\u001B[0;32m   1259\u001B[0m     )\n\u001B[1;32m-> 1260\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m cast(ResponseT, \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrequest\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mopts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m)\u001B[49m)\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\scientificProjectXAI\\Lib\\site-packages\\openai\\_base_client.py:937\u001B[0m, in \u001B[0;36mSyncAPIClient.request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m    928\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mrequest\u001B[39m(\n\u001B[0;32m    929\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[0;32m    930\u001B[0m     cast_to: Type[ResponseT],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    935\u001B[0m     stream_cls: \u001B[38;5;28mtype\u001B[39m[_StreamT] \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    936\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m ResponseT \u001B[38;5;241m|\u001B[39m _StreamT:\n\u001B[1;32m--> 937\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    938\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    939\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    940\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    941\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    942\u001B[0m \u001B[43m        \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining_retries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    943\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\scientificProjectXAI\\Lib\\site-packages\\openai\\_base_client.py:1026\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1024\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m retries \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_should_retry(err\u001B[38;5;241m.\u001B[39mresponse):\n\u001B[0;32m   1025\u001B[0m     err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mclose()\n\u001B[1;32m-> 1026\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_retry_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1027\u001B[0m \u001B[43m        \u001B[49m\u001B[43minput_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1028\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1029\u001B[0m \u001B[43m        \u001B[49m\u001B[43mretries\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1030\u001B[0m \u001B[43m        \u001B[49m\u001B[43merr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mheaders\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1031\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1032\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1033\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1035\u001B[0m \u001B[38;5;66;03m# If the response is streamed then we need to explicitly read the response\u001B[39;00m\n\u001B[0;32m   1036\u001B[0m \u001B[38;5;66;03m# to completion before attempting to access the response text.\u001B[39;00m\n\u001B[0;32m   1037\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mis_closed:\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\scientificProjectXAI\\Lib\\site-packages\\openai\\_base_client.py:1075\u001B[0m, in \u001B[0;36mSyncAPIClient._retry_request\u001B[1;34m(self, options, cast_to, remaining_retries, response_headers, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1071\u001B[0m \u001B[38;5;66;03m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001B[39;00m\n\u001B[0;32m   1072\u001B[0m \u001B[38;5;66;03m# different thread if necessary.\u001B[39;00m\n\u001B[0;32m   1073\u001B[0m time\u001B[38;5;241m.\u001B[39msleep(timeout)\n\u001B[1;32m-> 1075\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_request\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1076\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1077\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast_to\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcast_to\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1078\u001B[0m \u001B[43m    \u001B[49m\u001B[43mremaining_retries\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mremaining\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1079\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1080\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstream_cls\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstream_cls\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1081\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\miniconda3\\envs\\scientificProjectXAI\\Lib\\site-packages\\openai\\_base_client.py:1041\u001B[0m, in \u001B[0;36mSyncAPIClient._request\u001B[1;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001B[0m\n\u001B[0;32m   1038\u001B[0m         err\u001B[38;5;241m.\u001B[39mresponse\u001B[38;5;241m.\u001B[39mread()\n\u001B[0;32m   1040\u001B[0m     log\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mRe-raising status error\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m-> 1041\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_status_error_from_response(err\u001B[38;5;241m.\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m   1043\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_process_response(\n\u001B[0;32m   1044\u001B[0m     cast_to\u001B[38;5;241m=\u001B[39mcast_to,\n\u001B[0;32m   1045\u001B[0m     options\u001B[38;5;241m=\u001B[39moptions,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1049\u001B[0m     retries_taken\u001B[38;5;241m=\u001B[39moptions\u001B[38;5;241m.\u001B[39mget_max_retries(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmax_retries) \u001B[38;5;241m-\u001B[39m retries,\n\u001B[0;32m   1050\u001B[0m )\n",
      "\u001B[1;31mBadRequestError\u001B[0m: Error code: 400 - {'error': {'message': 'Thread thread_XSPoNyv5dfB5Z6LN0OFwfKCb already has an active run run_LvQEUk4iNmPtAj5BFf3WbxBu.', 'type': 'invalid_request_error', 'param': None, 'code': None}}"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T09:14:07.090575Z",
     "start_time": "2024-08-21T09:13:36.627004Z"
    }
   },
   "cell_type": "code",
   "source": [
    "actual_response_texts = []\n",
    "# run_ids.append(\"59598\")\n",
    "for run_id in run_ids:\n",
    "    thread_message = client.beta.threads.messages.list(\n",
    "        thread_id=thread.id,\n",
    "        run_id=run_id\n",
    "    )\n",
    "    # print(thread_message, \"\\n\")\n",
    "    if not thread_message.data:\n",
    "        # print(\"NONE for this\")\n",
    "        actual_response_text = \"\"\n",
    "    else:\n",
    "        actual_response_text = thread_message.data[0].content[0].text.value\n",
    "        \n",
    "    actual_response_texts.append(actual_response_text)\n",
    "# print(len(thread_messages_responses))\n",
    "# for j, i in enumerate(actual_response_texts):\n",
    "#     print(j, i, \"\\n\")\n"
   ],
   "id": "bce5e98ad0c2f165",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T21:04:51.949663Z",
     "start_time": "2024-08-20T21:04:51.946031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for i in actual_response_texts:\n",
    "#     print(i)"
   ],
   "id": "f9d34f5e6493a608",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is classified as normal.\n",
      "\n",
      "The following words may have inappropriate meanings: ***baby, white, touched***\n",
      "\n",
      "Explanation: The possible explanation is that the word/usage ***baby*** can sometimes refer to a person in a derogatory manner, while ***white*** may have racial connotations depending on the context, but here it seems to refer to the color of shoes. The word ***touched*** could imply unwanted physical contact in other contexts, but in this case, it appears to refer to the condition of the shoes.\n",
      "This text is classified as normal.\n",
      "\n",
      "The following words may have inappropriate meanings: ***womxn, offences, queer***\n",
      "\n",
      "Explanation: The possible explanation is that the term ***womxn*** is used as an inclusive alternative to \"women,\" which some may find controversial or inappropriate in certain contexts. The word ***offences*** refers to legal violations, which can be sensitive but is used in a factual manner here. The term ***queer*** can be considered offensive in some contexts; however, it is often embraced within the LGBTQ+ community as a positive identifier.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***nawt, niggers, ignoring***\n",
      "\n",
      "Explanation: The term ***niggers*** is a highly offensive racial slur directed at Black individuals, which is widely recognized as hate speech. The word ***nawt*** is a colloquial variation of \"not,\" which, while not inherently inappropriate, is used in a context that contributes to the overall offensive tone. The word ***ignoring*** itself is neutral, but its usage in this context amplifies the derogatory sentiment expressed in the statement.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***chinese, ching chong, pakistani***\n",
      "\n",
      "Explanation: The term ***ching chong*** is a derogatory and mocking phrase used to refer to people of Chinese descent, which is considered hate speech. Additionally, the mention of ***pakistani*** in a context that seems to stereotype or generalize based on ethnicity can also be seen as offensive. The overall tone of the text implies a prejudiced viewpoint, contributing to its classification as hate speech.\n",
      "This text is classified as offensive.\n",
      "\n",
      "The following words may have inappropriate meanings: ***bitch, hate, white***\n",
      "\n",
      "Explanation: The term ***bitch*** is used here in a derogatory manner to refer to a woman, which is offensive. The word ***hate*** expresses a strong negative sentiment, indicating animosity towards a specific group. The reference to ***white*** in conjunction with the derogatory term suggests a racial component to the offense, making the statement more inflammatory and contributing to its classification as offensive.\n",
      "[]\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-20T20:12:49.494055Z",
     "start_time": "2024-08-20T20:12:49.490956Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for i in responses:\n",
    "#     print(i)"
   ],
   "id": "fd372be84d91f6aa",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This text is classified as normal.\n",
      "\n",
      "The following words may have inappropriate meanings: ***baby, white, j, nikes***\n",
      "\n",
      "Explanation: The possible explanation is that the word/usage ***baby*** refers to a child or loved one, which is generally neutral, while ***white*** in this context is likely referring to the color of shoes and does not carry an inappropriate meaning. The terms ***j*** and ***nikes*** refer to specific sneaker models, which are also neutral in this context.\n",
      "This text is classified as normal.\n",
      "\n",
      "The following words may have inappropriate meanings: ***womxn, trans, lesbian, bisexual, queer***\n",
      "\n",
      "Explanation: The possible explanation is that the terms ***womxn***, ***trans***, ***lesbian***, ***bisexual***, and ***queer*** are used to describe various identities and orientations within the LGBTQ+ community. In this context, they are used to advocate for inclusivity and do not carry inappropriate meanings.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***nawt, niggers, ignoring***\n",
      "\n",
      "Explanation: The term ***niggers*** is a derogatory racial slur that is historically used to demean and dehumanize Black individuals. Its presence in the text directly contributes to the classification of hate speech. The word ***nawt*** is a colloquial variation of \"not,\" which may not be inappropriate but is used in a context that includes hate speech. The word ***ignoring***, while neutral, is used in a confrontational context here.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***chinese, ching chong, pakistani***\n",
      "\n",
      "Explanation: The term ***ching chong*** is a derogatory phrase that is used to mock or belittle individuals of Chinese descent, which is offensive and contributes to the classification of hate speech. The mention of ***chinese*** in the context of mocking and the assumption about someone's nationality based on stereotypes further reinforces the hateful sentiment. While ***pakistani*** is a neutral term, its use in this context appears to be part of a dismissive stereotype, contributing to the overall negative tone of the message.\n",
      "This text is classified as offensive.\n",
      "\n",
      "The following words may have inappropriate meanings: ***bitch, white, bitches***\n",
      "\n",
      "Explanation: The term ***bitch*** is used as a derogatory term for a woman, which is offensive in this context. Additionally, the phrase ***white bitches*** generalizes and demeans women based on their race, contributing to the offensive nature of the text. The context of the comment about someone eating a burger also adds a mocking tone, reinforcing the overall negativity.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***raped, kike, exterminate, goyim***\n",
      "\n",
      "Explanation: The term ***raped*** is a violent and traumatic word that is used here in an inappropriate context, which can be deeply offensive. The word ***kike*** is a derogatory slur used against Jewish people, and its inclusion indicates a clear expression of hate. The phrase ***exterminate the goyim*** promotes violence and hatred against non-Jewish individuals, further solidifying the hateful nature of the text. Overall, the language used is inflammatory and promotes harmful stereotypes and ideologies.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***nigger, trainee, doctors, scary***\n",
      "\n",
      "Explanation: The term ***nigger*** is a highly offensive racial slur used to demean Black individuals, which is a clear indicator of hate speech. The phrase implies a negative stereotype about the abilities of Black trainee doctors, which perpetuates harmful prejudices. While the words ***trainee*** and ***doctors*** are neutral, their use in this derogatory context contributes to the overall hateful sentiment of the message. The word ***scary*** is used to express fear based on racial bias, further reinforcing the text's hateful nature.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***nogs, jews, dykes***\n",
      "\n",
      "Explanation: The term ***nogs*** is a derogatory term used to refer to Black individuals, which is clearly offensive and promotes hate. The word ***jews***, while a neutral term when used appropriately, is often employed in a context that can perpetuate negative stereotypes or discrimination, especially in this phrase. The term ***dykes*** is a slur used against lesbian women, which is derogatory and offensive. The overall context of the message suggests a disdainful attitude toward these groups, contributing to the classification as hate speech.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***nogs, jews, dykes***\n",
      "\n",
      "Explanation: The term ***nogs*** is a derogatory term used to refer to Black individuals, which is clearly offensive and promotes hate. The word ***jews***, while a neutral term when used appropriately, is often employed in a context that can perpetuate negative stereotypes or discrimination, especially in this phrase. The term ***dykes*** is a slur used against lesbian women, which is derogatory and offensive. The overall context of the message suggests a disdainful attitude toward these groups, contributing to the classification as hate speech.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***niglets, rapists, welfare, democrat***\n",
      "\n",
      "Explanation: The term ***niglets*** is a derogatory and dehumanizing term used to refer to Black children, which is offensive and promotes hate. The phrase suggests a harmful stereotype that equates Black individuals with criminal behavior and dependency on welfare, which is a racist generalization. The use of ***rapists*** in this context further perpetuates a narrative that unjustly vilifies a racial group. Overall, the language used is inflammatory and promotes harmful stereotypes, contributing to the classification of hate speech.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***multiculturalism, muzzie, minaret***\n",
      "\n",
      "Explanation: The term ***muzzies*** is a derogatory slang term used to refer to Muslims, which is offensive and promotes negative stereotypes about a religious group. The phrase suggests a disdain for multiculturalism and implies that Muslims are somehow detrimental to British society, which fosters division and hostility. The mention of converting ***Big Ben into a minaret*** further underscores an anti-Muslim sentiment, reinforcing the hateful nature of the text. Overall, the language used is inflammatory and promotes harmful stereotypes, contributing to the classification of hate speech.\n",
      "This text is classified as normal.\n",
      "\n",
      "The following words may have inappropriate meanings: ***niggas***\n",
      "\n",
      "Explanation: The term ***niggas*** is a colloquial variation of the racial slur ***nigger***, which can be considered offensive. However, in certain contexts, particularly within some communities, it may be used as a term of endearment or camaraderie among friends. In this specific instance, the overall context appears to be light-hearted and humorous, suggesting that it is not intended as hate speech, although it remains a contentious term. The rest of the text does not contain any overtly offensive language.\n",
      "This text is classified as normal.\n",
      "\n",
      "The following words may have inappropriate meanings: ***gay, chemicals***\n",
      "\n",
      "Explanation: The term ***gay*** is typically a neutral descriptor for sexual orientation; however, in this context, it is used in a derogatory manner implying that being gay is associated with negative attributes, which can be considered offensive. The phrase ***gay chemicals*** suggests a nonsensical or pejorative belief about the influence of gay individuals on certain substances or behaviors. While the text contains potentially problematic language, it does not rise to the level of hate speech based on the overall context and intent.\n",
      "This text is classified as normal.\n",
      "\n",
      "The following words may have inappropriate meanings: ***closet, gay, obsessed***\n",
      "\n",
      "Explanation: The term ***closet*** refers to someone who is not openly expressing their sexual orientation, which is a neutral term in this context. The word ***gay*** is also neutral when used to describe someone's sexual orientation. The term ***obsessed*** could imply an unhealthy fixation, but it does not inherently carry a negative connotation. Overall, while the text may raise some concerns about the implications of obsession, it does not contain overtly offensive language or hate speech.\n",
      "This text is classified as offensive.\n",
      "\n",
      "The following words may have inappropriate meanings: ***hoes, thots***\n",
      "\n",
      "Explanation: The terms ***hoes*** and ***thots*** are derogatory slang used to describe women in a disrespectful manner, implying promiscuity. Such language is objectifying and perpetuates negative stereotypes about women's sexual behavior. Although the overall sentiment may be light-hearted, the use of these terms contributes to an offensive tone, hence the classification as offensive rather than hate speech.\n",
      "This text is classified as normal.\n",
      "\n",
      "The following words may have inappropriate meanings: ***niggas***\n",
      "\n",
      "Explanation: The term ***niggas*** is a colloquial variation of the racial slur ***nigger***. While it can be considered offensive, it is often used within certain communities as a term of camaraderie among friends. In this context, it seems to be used informally and does not appear to carry a hateful intent. The overall message is a cautionary statement about relationships, which is not inherently offensive despite the use of the term.\n",
      "This text is classified as normal.\n",
      "\n",
      "The following words may have inappropriate meanings: ***niggas***\n",
      "\n",
      "Explanation: The term ***niggas*** is a colloquial variation of the racial slur ***nigger***, which can be considered offensive. However, in certain contexts, particularly within some communities, it may be used as a term of endearment or camaraderie. In this instance, the use of the term seems to be part of a humorous or informal expression rather than a hateful statement. The phrase ***poozingi*** appears to be a playful or slang term for a male body part, which does not carry an offensive connotation in this context. Overall, while the text contains potentially problematic language, it does not rise to the level of hate speech based on the overall context and intent.\n",
      "This text is classified as normal.\n",
      "\n",
      "The following words may have inappropriate meanings: ***nazi, white supremacist***\n",
      "\n",
      "Explanation: The terms ***nazi*** and ***white supremacist*** refer to specific ideologies associated with hate and discrimination. However, in this context, they are used to inquire about the actions or beliefs of an individual (the old lady) and do not express hate or offense themselves. The text appears to be asking a question rather than promoting hate speech or offensive content. Therefore, it is classified as normal.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***negress, white, black, love, nobama***\n",
      "\n",
      "Explanation: The term ***negress*** is an outdated and derogatory term used to refer to Black women, which is offensive and perpetuates racial stereotypes. The phrase implies a negative sentiment towards both white women and Black women, suggesting that the subject's marriage choice is a result of rejection by white women, which reinforces harmful racial narratives. The use of ***nobama*** appears to be a derogatory reference to former President Obama, further indicating a disdainful attitude. Overall, the language used in this text promotes racial division and negative stereotypes, contributing to its classification as hate speech.\n",
      "This text is classified as hate speech.\n",
      "\n",
      "The following words may have inappropriate meanings: ***kike***\n",
      "\n",
      "Explanation: The term ***kike*** is a derogatory slur used against Jewish people, which is highly offensive and promotes hate. Its use in this context indicates a disdainful attitude towards individuals or groups associated with drug use, suggesting that the speaker is linking drug use to Jewish identity in a negative manner. This reinforces harmful stereotypes and prejudices, contributing to the classification of the text as hate speech.\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T09:15:23.954979Z",
     "start_time": "2024-08-21T09:15:23.923729Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import re\n",
    "\n",
    "important_words_GPT = []\n",
    "def extract_inappropriate_words(text):\n",
    "    pattern = r\"\\*\\*\\*(.*?)\\*\\*\\*\"\n",
    "    matches = re.findall(pattern, text)\n",
    "    \n",
    "    if matches:\n",
    "        words_list = [word.strip() for word in matches[0].split(',')]\n",
    "        return words_list\n",
    "    return []\n",
    "\n",
    "for actual_response_text in actual_response_texts:\n",
    "    # print(type(actual_response_text))\n",
    "    important_words = extract_inappropriate_words(actual_response_text)\n",
    "    important_words_GPT.append(important_words)\n",
    "    \n",
    "for i in important_words_GPT:\n",
    "    print(i)\n",
    "\n",
    "# print(len(important_words_GPT))"
   ],
   "id": "523f539915ea1cd2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baby', 'white', 'touched']\n",
      "['womxn', 'offences', 'sexual']\n",
      "['nawt', 'niggers', 'ignoring']\n",
      "['chinese', 'ching chong', 'pakistani']\n",
      "['bitch', 'hate', 'white']\n",
      "['raped', 'kike', 'exterminate', 'goyim']\n",
      "['nigger', 'trainee', 'scary']\n",
      "['nogs', 'jews', 'dykes']\n",
      "['guilty', 'jew', 'nigger', 'lover']\n",
      "['abortion', 'niglets', 'rapists', 'welfare', 'democrat']\n",
      "['multiculturalism', 'muzzie', 'minaret']\n",
      "['niggas']\n",
      "['gay', 'chemicals']\n",
      "['closet', 'gay', 'obsessed']\n",
      "['hoes', 'thots']\n",
      "['niggas']\n",
      "['niggas']\n",
      "['nazi', 'white supremacist']\n",
      "['negress', 'white', 'black', 'love', 'nobama']\n",
      "['kike']\n",
      "['israel']\n",
      "['kps', 'islam', 'terrorist', 'butured']\n",
      "['islamophobia', 'white supremacy']\n",
      "['kike']\n",
      "['nigger', 'nazis']\n",
      "['beaner']\n",
      "['albino', 'nigger']\n",
      "['spic']\n",
      "['kike']\n",
      "['spic']\n",
      "['moslem', 'welfare', 'taxpayer', 'homeless vets']\n",
      "['negro', 'moslem']\n",
      "['dyke']\n",
      "['sumbitch']\n",
      "['bitch']\n",
      "['black']\n",
      "['bitch']\n",
      "['bitches', 'niggas']\n",
      "['queer', 'trans']\n",
      "['white folks']\n",
      "['faggots']\n",
      "['blacks', 'jews', 'white']\n",
      "['invaders']\n",
      "['scheming', 'fraud', 'hillbilly', 'cult']\n",
      "['unknown', 'white']\n",
      "['faggot']\n",
      "['shithole']\n",
      "['pie rats', 'black']\n",
      "['none']\n",
      "['hoes']\n",
      "['niggas', 'females']\n",
      "['nigga', 'bitches']\n",
      "['sluttiest']\n",
      "['nazis', 'raped', 'tortured']\n",
      "['trans', 'color', 'disabled', 'queer', 'latina']\n",
      "['negroes']\n",
      "['motherfucking', 'faggot']\n",
      "['redneck']\n",
      "['maga fags', 'nigger']\n",
      "['faggot', 'dike']\n",
      "['nigger']\n",
      "['nigger']\n",
      "['kike']\n",
      "[]\n",
      "['nigger', 'dogs', 'mudshark']\n",
      "[]\n",
      "['kike']\n",
      "[]\n",
      "[]\n",
      "[]\n",
      "['filthy', 'dykes']\n",
      "['coons']\n",
      "['muzzies', 'scum']\n",
      "['kike']\n",
      "['wetback']\n",
      "['kike']\n",
      "['none']\n",
      "['none']\n",
      "['stupid', 'hoe']\n",
      "['nude', 'sex toys']\n",
      "['none']\n",
      "['woah', 'extravagant', 'hoes']\n",
      "['hoe']\n",
      "['bitches', 'niggas']\n",
      "['none']\n",
      "['niggas']\n",
      "[]\n",
      "['none']\n",
      "['none']\n",
      "['slut']\n",
      "['none']\n",
      "['muzrat']\n",
      "['none']\n",
      "['none']\n",
      "['faggots', 'fag', 'paypig', 'cash master', 'cash slave']\n",
      "['ghetto']\n",
      "['nigger', 'niggers']\n",
      "['bitch']\n",
      "['nigger', 'jews']\n",
      "['nigger']\n",
      "100\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T09:16:51.142437Z",
     "start_time": "2024-08-21T09:16:51.126735Z"
    }
   },
   "cell_type": "code",
   "source": [
    "spanr_count = 0\n",
    "spanr_total = 0\n",
    "cover_total = 0\n",
    "cover_correct = 0\n",
    "\n",
    "rationale_tokens_data = rationale_tokens_data_full[:100]\n",
    "\n",
    "for i in range(100):\n",
    "    if annotated_labels[i] == 1:\n",
    "        rationale_tokens = set(rationale_tokens_data[i])\n",
    "        # important_tokens = set(important_words_data[i][\"important_words\"])\n",
    "        important_tokens = set(important_words_GPT[i])\n",
    "\n",
    "        spanr_total += 1\n",
    "        if rationale_tokens & important_tokens:\n",
    "            # print(i, rationale_tokens & important_tokens)\n",
    "            spanr_count += 1\n",
    "            cover_correct += len(rationale_tokens & important_tokens)\n",
    "        cover_total += len(rationale_tokens)\n",
    "\n",
    "if cover_total > 0:\n",
    "    cover = cover_correct / cover_total\n",
    "else:\n",
    "    cover = 0\n",
    "\n",
    "spanr = spanr_count / spanr_total\n",
    "\n",
    "print(f\"SpanR: {spanr}\")\n",
    "print(f\"Cover: {cover}\")\n",
    "print(spanr_total, spanr_count)"
   ],
   "id": "a12457ed509a8811",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpanR: 0.8478260869565217\n",
      "Cover: 0.3155339805825243\n",
      "46 39\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "41099bf11b536a83"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

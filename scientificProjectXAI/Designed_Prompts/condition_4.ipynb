{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import original post tokens",
   "id": "f36074011d602c01"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T07:29:20.151618Z",
     "start_time": "2024-12-18T07:29:20.058452Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "with open('../data/original_sentences.json', 'r', encoding='utf-8') as f:\n",
    "    original_sentences = json.load(f)\n",
    "    \n",
    "# with open('../data/important_words_abusive_SHAP.json', 'r') as f:\n",
    "#     important_words_abusive_SHAP = json.load(f)\n",
    "    \n",
    "with open('../data/important_words_SHAP.json', 'r') as f:\n",
    "    important_words_SHAP = json.load(f)\n",
    "    \n",
    "with open('../data/classified_results.json', 'r') as f:\n",
    "    classified_results = json.load(f)"
   ],
   "id": "b65d0391a0b56820",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T07:29:20.156855Z",
     "start_time": "2024-12-18T07:29:20.153614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(len(important_words_SHAP))\n",
    "# for i in range(0, len(important_words_abusive_SHAP[:100])):\n",
    "#     print(i, original_sentences[i], \"\\n\", important_words_abusive_SHAP[i], \"\\n\", important_words_SHAP[i], \"\\n\")"
   ],
   "id": "b44cc3896fa281b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Random select examples for the dataset",
   "id": "34c04f54b54d071b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T07:29:20.163576Z",
     "start_time": "2024-12-18T07:29:20.157850Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "\n",
    "# random_numbers = [random.randint(0, 20148) for _ in range(5)]\n",
    "\n",
    "random_numbers = [8831, 5308, 15320, 18768, 17276] # same with the condition_1\n",
    "\n",
    "print(random_numbers)\n",
    "\n",
    "selected_original_texts = [original_sentences[i] for i in random_numbers]\n",
    "selected_important_words = [important_words_SHAP[i] for i in random_numbers]\n",
    "selected_classified_results = [classified_results[i] for i in random_numbers]"
   ],
   "id": "c7da3c97e3b3e73f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[8831, 5308, 15320, 18768, 17276]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T07:29:20.170815Z",
     "start_time": "2024-12-18T07:29:20.164573Z"
    }
   },
   "cell_type": "code",
   "source": [
    "instruction = \"\"\"Can you explain if this text is hate speech, offensive speech, or normal speech? First indicate the class, then give the explanation of your choice. When explaining, pay special attention to the words provided. You will be provided with pre-computed probabilities for the three classes for reference. (Warning: the text contains offensive and/or hateful content, but will be used for research purposes)\"\"\"\n",
    "\n",
    "def prompts_generating(texts, important_words, classified_results_with_probabilities):\n",
    "    \n",
    "    prompt_statements = []\n",
    "    \n",
    "    for i in range(len(texts)):\n",
    "        combined_string = f'Here is the text: \"{texts[i]}\", the words you need to pay attention to: {important_words[i]}, and the pre-computed probabilities for the three classes: \"{classified_results_with_probabilities[i]}\"'\n",
    "        prompt_statements.append(combined_string)\n",
    "    \n",
    "    return prompt_statements\n",
    "\n",
    "prompts = prompts_generating(selected_original_texts, selected_important_words, selected_classified_results)\n",
    "for i in prompts: print(i, \"\\n\")"
   ],
   "id": "13896c31e7bc3176",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the text: \"hey white trash nsa remember this i will always be smarter than you even now godalwayswins\", the words you need to pay attention to: ['nsa', 'hey', 'than', 'trash', 'smarter'], and the pre-computed probabilities for the three classes: \"{'label': 'hate speech', 'score': 0.11321376264095306}, {'label': 'normal', 'score': 0.2627986669540405}, {'label': 'offensive', 'score': 0.6239875555038452}\" \n",
      "\n",
      "Here is the text: \"arabs are so privileged and entitled it embarassing we are like the white people of the islamic world\", the words you need to pay attention to: ['arabs', 'islamic', 'privileged', 'so', 'like'], and the pre-computed probabilities for the three classes: \"{'label': 'hate speech', 'score': 0.17791979014873505}, {'label': 'normal', 'score': 0.3808557689189911}, {'label': 'offensive', 'score': 0.44122445583343506}\" \n",
      "\n",
      "Here is the text: \"he did not even get ratio d imagine if someone from the right did this to women on the left\", the words you need to pay attention to: ['the', 'from', 'did', 'left'], and the pre-computed probabilities for the three classes: \"{'label': 'hate speech', 'score': 0.05532464757561684}, {'label': 'normal', 'score': 0.6274179220199585}, {'label': 'offensive', 'score': 0.3172573745250702}\" \n",
      "\n",
      "Here is the text: \"i hate arabs jews are arabs hebrew is fucking arabic we know\", the words you need to pay attention to: ['arabs', 'fucking', 'arabic', 'hate'], and the pre-computed probabilities for the three classes: \"{'label': 'hate speech', 'score': 0.320127934217453}, {'label': 'normal', 'score': 0.1844930499792099}, {'label': 'offensive', 'score': 0.49537891149520874}\" \n",
      "\n",
      "Here is the text: \"joe biden own words are proof that the left resorts to violence when they do not like something said by others that the tolerant left for you from a legal standpoint nonthreatening words are never justification for assault battery\", the words you need to pay attention to: ['battery', 'justification', 'words', 'assault'], and the pre-computed probabilities for the three classes: \"{'label': 'hate speech', 'score': 0.04251330345869064}, {'label': 'normal', 'score': 0.7225062251091003}, {'label': 'offensive', 'score': 0.23498044908046722}\" \n",
      "\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T07:29:29.929772Z",
     "start_time": "2024-12-18T07:29:20.172823Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "class ResponseFormat(BaseModel):\n",
    "    output_class: str\n",
    "    # output_indicators: list[str]\n",
    "    output_explanation: str\n",
    "\n",
    "def complete_chat_completion(prompt):\n",
    "    completion = client.beta.chat.completions.parse(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "          {\"role\": \"system\", \"content\": instruction},\n",
    "          {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        response_format={\n",
    "            \"type\": \"json_schema\",\n",
    "            \"json_schema\": {\n",
    "                \"name\": \"ResponseFormat\",\n",
    "                \"schema\": {\n",
    "                    \"type\": \"object\",\n",
    "                    \"properties\": {\n",
    "                        \"output_class\": {\"type\": \"string\"},\n",
    "                        # \"output_indicators\": {\n",
    "                        #     \"type\": \"array\",\n",
    "                        #     \"items\": {\"type\": \"string\"}\n",
    "                        # },\n",
    "                        \"output_explanation\": {\"type\": \"string\"}\n",
    "                    },\n",
    "                    # \"required\": [\"output_class\", \"output_indicators\", \"output_explanation\"],\n",
    "                    \"required\": [\"output_class\", \"output_explanation\"],\n",
    "                    \"additionalProperties\": False\n",
    "                },\n",
    "                \"strict\": True\n",
    "            }\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    parsed_content = json.loads(completion.choices[0].message.content)\n",
    "    formatted_response_content = ResponseFormat(**parsed_content)\n",
    "    \n",
    "    return formatted_response_content\n",
    "\n",
    "response_contents = [complete_chat_completion(prompt) for prompt in prompts]\n",
    "# print(response_contents)"
   ],
   "id": "3e77383dd91c0936",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T07:29:29.936337Z",
     "start_time": "2024-12-18T07:29:29.931781Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for i in response_contents: print(i, \"\\n\")\n",
    "\n",
    "for i in range(len(prompts)): \n",
    "    print(prompts[i], \"\\n\")\n",
    "    print(response_contents[i], \"\\n\")\n",
    "    print(\"*\" * 130)"
   ],
   "id": "b1a0c227d6a6cdbd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is the text: \"hey white trash nsa remember this i will always be smarter than you even now godalwayswins\", the words you need to pay attention to: ['nsa', 'hey', 'than', 'trash', 'smarter'], and the pre-computed probabilities for the three classes: \"{'label': 'hate speech', 'score': 0.11321376264095306}, {'label': 'normal', 'score': 0.2627986669540405}, {'label': 'offensive', 'score': 0.6239875555038452}\" \n",
      "\n",
      "output_class='offensive' output_explanation=\"The text contains derogatory language, specifically referring to someone as 'white trash', which is a term often used to demean and dehumanize individuals based on socioeconomic status and race. Additionally, the phrase implies hostility and superiority ('I will always be smarter than you'), which can be considered offensive. Given the strong negative connotation of 'trash' and the overall tone of the message, this text fits the category of offensive speech rather than hate speech or normal speech.\" \n",
      "\n",
      "**********************************************************************************************************************************\n",
      "Here is the text: \"arabs are so privileged and entitled it embarassing we are like the white people of the islamic world\", the words you need to pay attention to: ['arabs', 'islamic', 'privileged', 'so', 'like'], and the pre-computed probabilities for the three classes: \"{'label': 'hate speech', 'score': 0.17791979014873505}, {'label': 'normal', 'score': 0.3808557689189911}, {'label': 'offensive', 'score': 0.44122445583343506}\" \n",
      "\n",
      "output_class='offensive' output_explanation=\"The text contains derogatory language and makes sweeping generalizations about a group, which can be interpreted as offensive. The use of terms like 'privileged' and 'entitled' along with the context of comparing 'arabs' to 'white people' indicates a negative characterization that is not only critical but also reinforces stereotypes. Although it may not directly incite hatred, it is still offensive in nature, reflecting a prejudiced viewpoint.\" \n",
      "\n",
      "**********************************************************************************************************************************\n",
      "Here is the text: \"he did not even get ratio d imagine if someone from the right did this to women on the left\", the words you need to pay attention to: ['the', 'from', 'did', 'left'], and the pre-computed probabilities for the three classes: \"{'label': 'hate speech', 'score': 0.05532464757561684}, {'label': 'normal', 'score': 0.6274179220199585}, {'label': 'offensive', 'score': 0.3172573745250702}\" \n",
      "\n",
      "output_class='normal' output_explanation=\"The text discusses a hypothetical scenario involving political groups ('the right' and 'the left') but does not contain any hateful or offensive language directed at any individual or group. The words highlighted ('the', 'from', 'did', 'left') are common and do not contribute to any hate or offense. Instead, the text reflects a viewpoint on political dynamics without disparaging anyone.\" \n",
      "\n",
      "**********************************************************************************************************************************\n",
      "Here is the text: \"i hate arabs jews are arabs hebrew is fucking arabic we know\", the words you need to pay attention to: ['arabs', 'fucking', 'arabic', 'hate'], and the pre-computed probabilities for the three classes: \"{'label': 'hate speech', 'score': 0.320127934217453}, {'label': 'normal', 'score': 0.1844930499792099}, {'label': 'offensive', 'score': 0.49537891149520874}\" \n",
      "\n",
      "output_class='offensive' output_explanation=\"The text contains derogatory language ('i hate', 'fucking') directed at specific ethnic groups ('arabs', 'jews'). The use of 'hate' indicates strong negative feelings towards a group, while 'fucking' is a profanity that intensifies the offensive nature of the statement. Overall, the sentiment displayed is disrespectful and inflammatory, but it does not fit the more extreme criteria for hate speech, focusing instead on offensive speech.\" \n",
      "\n",
      "**********************************************************************************************************************************\n",
      "Here is the text: \"joe biden own words are proof that the left resorts to violence when they do not like something said by others that the tolerant left for you from a legal standpoint nonthreatening words are never justification for assault battery\", the words you need to pay attention to: ['battery', 'justification', 'words', 'assault'], and the pre-computed probabilities for the three classes: \"{'label': 'hate speech', 'score': 0.04251330345869064}, {'label': 'normal', 'score': 0.7225062251091003}, {'label': 'offensive', 'score': 0.23498044908046722}\" \n",
      "\n",
      "output_class='normal' output_explanation=\"The text presents an opinion on political matters, specifically criticizing actions attributed to a political group (the left). The use of terms like 'battery', 'justification', 'words', and 'assault' relates to legal terminology in the context of discussing behaviors rather than directing hate or offense towards a specific individual or group. Despite its critical tone, it does not dehumanize or incite violence against any group, hence it falls under the category of normal speech.\" \n",
      "\n",
      "**********************************************************************************************************************************\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-18T07:29:29.940376Z",
     "start_time": "2024-12-18T07:29:29.937344Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "29dcc8269eae1407",
   "outputs": [],
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

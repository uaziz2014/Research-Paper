{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ada109b9668710f",
   "metadata": {},
   "source": [
    "Prediction Model Importing"
   ]
  },
  {
   "cell_type": "code",
   "id": "596e06178e241747",
   "metadata": {},
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import transformers\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain\")\n",
    "# model = AutoModelForSequenceClassification.from_pretrained(\"Hate-speech-CNERG/bert-base-uncased-hatexplain\")\n",
    "# \n",
    "# # build a pipeline object to do predictions\n",
    "# classifier = transformers.pipeline(\n",
    "#     \"text-classification\",\n",
    "#     model=model,\n",
    "#     tokenizer=tokenizer,\n",
    "#     device=0,\n",
    "#     return_all_scores=True\n",
    "# )\n",
    "\n",
    "classifier = transformers.pipeline(\"text-classification\", model=\"Hate-speech-CNERG/bert-base-uncased-hatexplain\", device=0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "86e1a3dd5f568bc2",
   "metadata": {},
   "source": [
    "Extract data from json file"
   ]
  },
  {
   "cell_type": "code",
   "id": "a6ecca1e7778febe",
   "metadata": {},
   "source": [
    "import json\n",
    "\n",
    "json_file = '../data/dataset.json'\n",
    "\n",
    "def extract_data(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    sentences = []\n",
    "    abuse_flags = []\n",
    "    original_post_tokens = []\n",
    "    rationale_tokens_list = []\n",
    "    \n",
    "    for key, entry in data.items():\n",
    "\n",
    "        post_tokens = entry['post_tokens']\n",
    "        original_post_tokens.append(entry['post_tokens'])\n",
    "        sentence = \" \".join(post_tokens)\n",
    "\n",
    "        labels = [annotator['label'] for annotator in entry['annotators']]\n",
    "        if sum(label != \"normal\" for label in labels) >= 2:\n",
    "            abuse_label = 1  # Abusive\n",
    "        else:\n",
    "            abuse_label = 0  # normal\n",
    "        \n",
    "        sentences.append(sentence)\n",
    "        abuse_flags.append(abuse_label)\n",
    "        \n",
    "        if abuse_label == 1:\n",
    "            rationale_tokens = set()\n",
    "            rationales = entry['rationales']\n",
    "            for rationale in rationales:\n",
    "                for i, val in enumerate(rationale):\n",
    "                    # if i >= len(post_tokens):\n",
    "                    #     print(entry['post_id'], i, len(post_tokens))\n",
    "                    if i < len(post_tokens) and val == 1:  # 确保索引 i 不超出 post_tokens 的长度\n",
    "                        rationale_tokens.add(post_tokens[i])\n",
    "            rationale_tokens_list.append(list(rationale_tokens))  # 将集合转换为列表存储\n",
    "        else:\n",
    "            rationale_tokens_list.append([])  # 非abusive时添加一个空列表\n",
    "        \n",
    "    return sentences, abuse_flags, original_post_tokens, rationale_tokens_list\n",
    "\n",
    "original_sentences, annotated_labels, original_post_tokens, rationale_tokens_list = extract_data(json_file) #length = 20148\n",
    "\n",
    "# with open(\"../data/rationale_tokens_list.json\", \"w\") as f:\n",
    "#     json.dump(rationale_tokens_list, f, indent=4)\n",
    "# print(len(original_sentences)) 20148\n",
    "# print(annotated_labels[:5])\n",
    "# print(classifier(original_sentences[:5], return_all_scores=True))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a94e3c5e265e1e88",
   "metadata": {},
   "source": [
    "# from numba import cuda\n",
    "# device = cuda.get_current_device()\n",
    "# device.reset()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8693c6378fdc133b",
   "metadata": {},
   "source": [
    "SHAP calculating and saving function"
   ]
  },
  {
   "cell_type": "code",
   "id": "70bdd8f40f1217cf",
   "metadata": {},
   "source": [
    "import shap\n",
    "import json\n",
    "    \n",
    "explainer = shap.Explainer(model=classifier)\n",
    "\n",
    "def save_shap_values(shap_values, file_name):\n",
    "    shap_dict = {\n",
    "        \"values\": [arr.tolist() for arr in shap_values.values],\n",
    "        \"base_values\": shap_values.base_values.tolist(),\n",
    "        \"data\": [arr.tolist() for arr in shap_values.data]\n",
    "    }\n",
    "    \n",
    "    with open('../SHAP_values/' + file_name, \"w\") as f:\n",
    "        json.dump(shap_dict, f)\n",
    "        "
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "47a2fa36f40a9105",
   "metadata": {},
   "source": [
    "Calculate SHAP values per batch"
   ]
  },
  {
   "cell_type": "code",
   "id": "5fd7a89434fc3603",
   "metadata": {},
   "source": [
    "batch_size = 148\n",
    "end_point = 20148\n",
    "start_point = 20000\n",
    "\n",
    "for i in range(start_point, end_point, batch_size):\n",
    "    batch_sentences = original_sentences[i:i + batch_size]\n",
    "\n",
    "    shap_values = explainer(batch_sentences)\n",
    "\n",
    "    file_name = f\"shap_values_{i}_to_{i + len(batch_sentences)}.json\"\n",
    "    save_shap_values(shap_values, file_name)\n",
    "\n",
    "\n",
    "    print(f\"Saved SHAP values for sentences {i} to {i + len(batch_sentences)} in {file_name}\")\n",
    "\n",
    "#tqdm(pipe(dataset, batch_size=batch_size), total=len(datase))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "350faf8362db35fa",
   "metadata": {},
   "source": [
    "Visualize the SHAP top 10 for verification"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# print(original_sentences[113])",
   "id": "1a3ae332b45ef1d",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "36f5a16e81b022a6",
   "metadata": {},
   "source": [
    "# shap_values = explainer(original_sentences[113:115])\n",
    "# shap.plots.text(shap_values)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d6d8fc88ce9344e6",
   "metadata": {},
   "source": [
    "# html_file = shap.plots.text(shap_values, display=False)\n",
    "# with open(\"shap_output.html\", \"w\", encoding=\"utf-8\") as file:\n",
    "#     file.write(html_file)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Combine shap_values files into a single one",
   "id": "52a659685efe267e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:19:22.693550Z",
     "start_time": "2024-08-21T19:19:11.966384Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "shap_values_dir = 'SHAP_values/'\n",
    "\n",
    "combined_shap_values = {\n",
    "    \"values\": [],\n",
    "    \"base_values\": [],\n",
    "    \"data\": []\n",
    "}\n",
    "\n",
    "i = 0\n",
    "increment = 20000\n",
    "max_value = 20000\n",
    "\n",
    "while i <= max_value:\n",
    "    j = i + increment\n",
    "    file_name = f'shap_values_{i}_to_{j}.json'\n",
    "    file_path = shap_values_dir + file_name\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r') as f:\n",
    "            shap_data = json.load(f)\n",
    "            combined_shap_values[\"values\"].extend(shap_data[\"values\"])\n",
    "            combined_shap_values[\"base_values\"].extend(shap_data[\"base_values\"])\n",
    "            combined_shap_values[\"data\"].extend(shap_data[\"data\"])\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: {file_name} not found.\")\n",
    "\n",
    "    i += increment\n",
    "\n",
    "output_file = shap_values_dir + 'combined_shap_values.json'\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(combined_shap_values, f, indent=4)\n",
    "\n",
    "print(f\"All files have been combined and saved as {output_file}\")"
   ],
   "id": "1221fa3e52c107bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files have been combined and saved as ../SHAP_values/combined_shap_values.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-21T19:42:42.673214Z",
     "start_time": "2024-08-21T19:42:40.510251Z"
    }
   },
   "cell_type": "code",
   "source": [
    "with open('SHAP_values/combined_shap_values.json', 'r') as f:\n",
    "    combined_shap_values = json.load(f)\n",
    "    print(len(combined_shap_values[\"values\"]))"
   ],
   "id": "d4aaacdeede53eb3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20148\n"
     ]
    }
   ],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

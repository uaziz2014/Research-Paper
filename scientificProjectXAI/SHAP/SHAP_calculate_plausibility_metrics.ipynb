{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Extract the evidence in each instance provided by SHAP",
   "id": "68bccbbd2c949493"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:28:05.693297Z",
     "start_time": "2024-12-17T08:28:02.948698Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open(\"SHAP_values/combined_shap_values.json\", \"r\") as f:\n",
    "    shap_data = json.load(f)\n",
    "\n",
    "with open(\"../data/annotated_labels.json\", \"r\") as f:\n",
    "    annotated_labels = json.load(f)\n",
    "\n",
    "with open(\"../data/classified_labels.json\", \"r\") as f:\n",
    "    classified_labels = json.load(f)\n",
    "\n",
    "with open(\"../data/classified_labels_three_labels.json\", \"r\") as f:\n",
    "    classified_labels_three_labels = json.load(f)\n",
    "\n",
    "with open('../data/original_post_tokens.json', 'r', encoding='utf-8') as f:\n",
    "    original_post_tokens = json.load(f)"
   ],
   "id": "2e3bd9245b325494",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:28:05.698438Z",
     "start_time": "2024-12-17T08:28:05.695304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# shap_values = np.array(shap_data[\"values\"][15])\n",
    "# print(shap_values[:, 1])\n",
    "# print(np.array(shap_data[\"values\"][15]))"
   ],
   "id": "a08d9afb1fa4be63",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:28:05.712924Z",
     "start_time": "2024-12-17T08:28:05.699447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def map_shap_words_to_original(original_post_token,\n",
    "                               shap_words):  # map sub-words generated by SHAP to its possible original word\n",
    "    mapped_words = []\n",
    "\n",
    "    for shap_word in shap_words:\n",
    "        shap_word = shap_word.strip().lower()\n",
    "\n",
    "        # if len(shap_word) <= 1 or shap_word.isspace():\n",
    "        #     continue\n",
    "        if shap_word.isspace():\n",
    "            continue\n",
    "\n",
    "        best_match = None\n",
    "        min_distance = float('inf')\n",
    "\n",
    "        for word in original_post_token:\n",
    "            word_lower = word.lower()\n",
    "            if shap_word in word_lower:\n",
    "                start = word_lower.find(shap_word)\n",
    "                end = start + len(shap_word)\n",
    "                distance = min(abs(start), abs(len(word_lower) - end))\n",
    "                if distance < min_distance:\n",
    "                    min_distance = distance\n",
    "                    best_match = word\n",
    "\n",
    "        if best_match:\n",
    "            mapped_words.append(best_match)\n",
    "\n",
    "    return list(set(mapped_words))\n",
    "\n",
    "# def select_important_words_by_cumulative_contribution(shap_values, sentence_length, contribution_threshold=0.5):\n",
    "#     N = 5\n",
    "#     max_words = max(1, (sentence_length + N - 1) // N)  # 例如，当 N=5 时，长度5-9的句子选择2个单词，10-14选择3个\n",
    "# \n",
    "#     total_contribution = np.abs(shap_values).sum()\n",
    "#     # total_contribution = shap_values.sum()\n",
    "#     # print(total_contribution)\n",
    "#     sorted_indices = np.argsort(np.abs(shap_values))\n",
    "#     # print(sorted_indices)\n",
    "#     cumulative_contribution = 0.0\n",
    "#     selected_indices = []\n",
    "# \n",
    "#     for idx in sorted_indices:\n",
    "#         cumulative_contribution += np.abs(-shap_values[idx])\n",
    "#         selected_indices.append(idx)\n",
    "#         # if total_contribution == 0:\n",
    "#         #     break\n",
    "#         if cumulative_contribution / total_contribution >= contribution_threshold or len(selected_indices) >= max_words:\n",
    "#             break\n",
    "# \n",
    "#     return selected_indices"
   ],
   "id": "127e55b8b5b33e65",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:28:05.719052Z",
     "start_time": "2024-12-17T08:28:05.713918Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# important_words = []\n",
    "# \n",
    "# for idx, label in enumerate(classified_labels):\n",
    "#     if label == 1:\n",
    "#         shap_values = np.array(shap_data[\"values\"][idx])\n",
    "#         words = shap_data[\"data\"][idx]\n",
    "#         \n",
    "#         sentence_length = len(words)\n",
    "#         # if idx == 131:\n",
    "#         #     print(sentence_length)\n",
    "# \n",
    "#         selected_indices_hatespeech = select_important_words_by_cumulative_contribution(shap_values[:, 0], sentence_length)\n",
    "#         selected_indices_offensive = select_important_words_by_cumulative_contribution(shap_values[:, 2], sentence_length)\n",
    "# \n",
    "#         top_indices = list(set(selected_indices_hatespeech) | set(selected_indices_offensive))\n",
    "# \n",
    "#         important_words_for_entry = [words[i] for i in top_indices]\n",
    "#         mapped_words = map_shap_words_to_original(original_post_tokens[idx], important_words_for_entry)\n",
    "# \n",
    "#         # important_words.append({\n",
    "#         #     \"entry_index\": idx,\n",
    "#         #     \"important_words\": mapped_words,\n",
    "#         #     \"important_words_for_entry\": important_words_for_entry\n",
    "#         # })\n",
    "#         important_words.append(mapped_words)\n",
    "#     else:\n",
    "#         important_words.append([])\n",
    "\n",
    "# for instance in important_words:\n",
    "#     print(instance[\"entry_index\"], instance[\"important_words\"])\n",
    "#     if instance[\"entry_index\"] == 131:\n",
    "#         print(instance[\"important_words_for_entry\"])\n",
    "\n",
    "# save the important words into files\n",
    "# with open(\"../data/important_words_abusive_SHAP.json\", \"w\") as f:\n",
    "#     json.dump(important_words, f, indent=4)"
   ],
   "id": "fbf38a6bbc51e70",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Because the previous code cell does not generate the important words for normal speech, now I have modified it to include all the instances.",
   "id": "465ea227f511b38c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:28:05.724630Z",
     "start_time": "2024-12-17T08:28:05.721060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# def find_top_values(values_list):\n",
    "#     positive_indices = np.where(values_list > 0)[0]\n",
    "#     positive_values = values_list[positive_indices]\n",
    "#     top_5_indices_in_positive = np.argsort(positive_values)[-5:][::-1]\n",
    "#     top_5_original_indices = positive_indices[top_5_indices_in_positive]\n",
    "#     return top_5_original_indices\n",
    "# print(find_top_values(np.array([0.1, -0.3, 0.5, 0.2, 0, -0.1, 1.2, 0.4, 0.9, -0.5])))"
   ],
   "id": "7c6c108a6c8f3adc",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:28:07.061154Z",
     "start_time": "2024-12-17T08:28:05.725639Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import numpy as np\n",
    "\n",
    "def find_top_values(values_list):  #top 5 values' indexes\n",
    "    positive_indices = np.where(values_list > 0)[0]\n",
    "    positive_values = values_list[positive_indices]\n",
    "    top_5_indices_in_positive = np.argsort(positive_values)[-5:][::-1]\n",
    "    top_5_original_indices = positive_indices[top_5_indices_in_positive]\n",
    "    return top_5_original_indices\n",
    "\n",
    "\n",
    "def extract_important_words(shap_data_original, labels_three_classes):\n",
    "    important_words_all = []  # include all 3 classes\n",
    "    for index, labeled_class_int in enumerate(labels_three_classes):\n",
    "        extracted_shap_values = np.array(shap_data_original[\"values\"][index])\n",
    "        shap_words = shap_data_original[\"data\"][index]  # words or sub-words list, then we need to map it back to the original words\n",
    "\n",
    "        shap_values_of_one_class = extracted_shap_values[:, labeled_class_int]  # choose the right label column\n",
    "        selected_indices_for_the_class = find_top_values(shap_values_of_one_class)\n",
    "        important_words_for_single_entry = [shap_words[ind] for ind in selected_indices_for_the_class]\n",
    "        possible_mapped_words = map_shap_words_to_original(original_post_tokens[index],\n",
    "                                                           important_words_for_single_entry)\n",
    "        important_words_all.append(possible_mapped_words)\n",
    "    return important_words_all\n",
    "\n",
    "important_words = extract_important_words(shap_data, classified_labels_three_labels)\n",
    "\n",
    "# for i, word in enumerate(important_words[1500:1700]):\n",
    "#     print(i, word)\n",
    "# for instance in important_words:\n",
    "#     print(instance[\"entry_index\"], instance[\"important_words\"])\n",
    "#     if instance[\"entry_index\"] == 131:\n",
    "#         print(instance[\"important_words_for_entry\"])\n",
    "\n",
    "\n",
    "with open(\"../data/important_words_SHAP.json\", \"w\") as f:\n",
    "    json.dump(important_words, f, indent=4)"
   ],
   "id": "7b34df34cc12b826",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:28:07.165463Z",
     "start_time": "2024-12-17T08:28:07.062666Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import json\n",
    "\n",
    "# with open('../data/important_words_abusive_SHAP.json', 'r') as f:\n",
    "#     important_words_data = json.load(f)\n",
    "    \n",
    "with open('../data/important_words_SHAP.json', 'r') as f:\n",
    "    important_words_data = json.load(f)\n",
    "\n",
    "with open('../data/rationale_tokens_list.json', 'r') as f:\n",
    "    rationale_tokens_data = json.load(f)\n",
    "\n",
    "\n",
    "spanr_count = 0\n",
    "spanr_total = 0\n",
    "cover_total = 0\n",
    "cover_correct = 0\n",
    "\n",
    "# rationale_tokens_data = rationale_tokens_data_full[:11500]\n",
    "\n",
    "for i in range(len(annotated_labels)):\n",
    "    if annotated_labels[i] == 1:\n",
    "        rationale_tokens = set(rationale_tokens_data[i])\n",
    "        # important_tokens = set(important_words_data[i][\"important_words\"])\n",
    "        important_tokens = set(important_words_data[i])\n",
    "\n",
    "        spanr_total += 1  \n",
    "        if rationale_tokens & important_tokens:\n",
    "            # print(i, rationale_tokens & important_tokens)\n",
    "            spanr_count += 1\n",
    "            cover_correct += len(rationale_tokens & important_tokens)\n",
    "        cover_total += len(rationale_tokens)\n",
    "\n",
    "if cover_total > 0:\n",
    "    cover = cover_correct / cover_total\n",
    "else:\n",
    "    cover = 0\n",
    "\n",
    "spanr = spanr_count / spanr_total\n",
    "\n",
    "print(f\"SpanR: {spanr}\")\n",
    "print(f\"Cover: {cover}\")\n",
    "print(spanr_total, spanr_count)\n",
    "print(cover_correct, cover_total)\n"
   ],
   "id": "bc3b3262202bdb3c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SpanR: 0.8857629317334198\n",
      "Cover: 0.2702140557715624\n",
      "12334 10925\n",
      "25853 95676\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-17T08:28:07.169425Z",
     "start_time": "2024-12-17T08:28:07.166469Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "178aec833fd7f26b",
   "outputs": [],
   "execution_count": 7
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
